# zillow-scraper

# ğŸ¡ Advanced Zillow Scraper (ZIP Code-Based, Resumable, Scrapy-Only)

This is an advanced Zillow scraper built using **Scrapy only** â€” no Selenium, no Playwright, no headless browser.

Instead of parsing rendered HTML, it reads structured listing data directly from Zillow's embedded `__NEXT_DATA__` JSON, making it **faster, lighter, and easier to scale**.

---

## âš™ï¸ Features

- âœ… ZIP Codeâ€“based scraping
- âœ… Resumable by ZIP and page using `progress.json` and `done_zipcodes.txt`
- âœ… Extracts:
  - Listing URL, price, address, beds, baths, status
- âœ… Automatically follows pagination using `nextUrl`
- âœ… Custom headers to reduce bot detection
- âœ… Ideal for scraping **50K+ listings across Canada or the US**

---

## ğŸ“ Project Structure

```text
zillowspider.py         # Main spider logic
progress.json           # Tracks last page scraped per ZIP
done_zipcodes.txt       # Tracks completed ZIPs



ğŸ” Recovery System

This scraper can resume where it left off, even after crashes:

    progress.json: stores current page per ZIP

    done_zipcodes.txt: stores finished ZIPs

    On restart, it automatically skips ZIPs and resumes others from the last page

ğŸ› ï¸ How to Run

scrapy crawl zillowspider

(Or use scrapy crawl zillowspider -o results.json if not using FEEDS)
ğŸ“¦ Sample Output

{
  "zip_code": "K1A-0B1",
  "page": 2,
  "home_url": "https://www.zillow.com/homedetails/...",
  "price": "$849,000",
  "address": "123 Main St, Ottawa, ON",
  "status": "FOR_SALE"
}

ğŸ§  What I Learned

    How to reverse-engineer dynamic JavaScript websites using backend logic

    Why structured data is often easier to extract than raw HTML

    How to design robust scraping workflows with pause/resume and logging

ğŸ“Œ Future Plans

    Add ScraperAPI or proxy rotation support

    Store progress in SQLite or Redis

    Build a Streamlit dashboard to visualize the scraped data



---

## âœ… Next Steps

Let me know if you'd like:
- A custom project thumbnail or banner image for GitHub/LinkedIn
- Help setting up `FEEDS` to avoid `-o results.json`
- A Markdown badge setup (e.g., ![Scrapy](https://img.shields.io/badge/Scrapy-Framework-blue))




"This code is a demonstration project. Real-world deployments should include error handling, cloud scheduling, and optional proxy support, which I can provide based on your needs."
